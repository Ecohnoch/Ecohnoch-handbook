# 冻结部分参数

## 冻结普通层参数


只需要把相应的 `param.requires_grad` 设为 False，然后在优化器中不传入相应的参数即可

```
def freeze_model_by_weight_names(keys, model):
    """
    :param keys: list of weights names to be freeze
    :param model: model
    """
    for name, param in model.named_parameters():
        if name in keys:
            param.requires_grad = False

optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), opt.lr)
```

## 冻结 BN 层

由于 BN 层中训练和测试的行为不同，因此 BN 层仅仅冻结参数是不够的，还需要将相关的设为 `eval`

```
def set_bn_eval(module):
    classname = module.__class__.__name__
    if classname.find('BatchNorm') != -1 and \
            not module.weight.requires_grad:
        module.eval()

def train():
    model.train()  # train other layers
    model.apply(set_bn_eval)  # do not train BN
```

## 参考教程

- https://zhuanlan.zhihu.com/p/65439075
- https://www.zhihu.com/question/311095447/answer/589307812
- https://blog.csdn.net/qq_34914551/article/details/87699317
