# 分层学习率

与 TF 不同，torch 中非常方便地支持分层学习率，仅仅需要在初始化优化器的时候指定。

```python
# for DDP
pretrain_params = (param for name, param in model.named_parameters() if name[7:] in state_dict.keys())
other_params = (param for name, param in model.named_parameters() if name[7:] not in state_dict.keys())
optimizer = torch.optim.Adam([{'params': pretrain_params, 'lr': 0.1 * opt.lr},
                              {'params': other_params, 'lr': opt.lr}])
```

## 参考教程

- https://blog.csdn.net/qq_34914551/article/details/87699317
- https://blog.csdn.net/wangbin12122224/article/details/79949824